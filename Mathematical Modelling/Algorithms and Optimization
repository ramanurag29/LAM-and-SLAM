# 🔍 Mathematical Analysis & Optimization of SLAM Algorithms  
*A concise technical summary of six localization algorithms — from Dead-Reckoning to GraphSLAM.*

---

## 📘 Introduction
This document provides a structured walkthrough of six localization algorithms explored in this project.  
Each section includes:
- A short conceptual overview  
- Core mathematical formulation  
- Optimization ideology — how the method is tuned, stabilized, or improved in practice  

Together, they form the theoretical backbone of the implemented **Advanced Multi-Algorithm SLAM** pipeline.

---

## 1️⃣ Dead-Reckoning
### 🔹 Concept  
The simplest baseline method — integrates motion inputs to estimate pose incrementally.  

### 🔹 Mathematical Formulation  
\[
\hat{\mathbf{x}}_{t+1} = f(\hat{\mathbf{x}}_t, \mathbf{u}_t) = 
\begin{bmatrix}
x_t + v_t \cos\theta_t \Delta t \\
y_t + v_t \sin\theta_t \Delta t \\
\theta_t + \omega_t \Delta t
\end{bmatrix}
\]

### 🔹 Optimization Ideology  
Drift grows over time ($\mathcal{O}(t)$ for bias, $\mathcal{O}(\sqrt{t})$ for noise).  
Compensated by **periodic landmark correction**, **gyro bias calibration**, or **fusion with absolute sensors (e.g., GPS/LiDAR)**.

---

## 2️⃣ Extended Kalman Filter (EKF)
### 🔹 Concept  
Probabilistic filtering that linearizes nonlinear motion and measurement models.

### 🔹 Core Equations  
**Prediction Step:**
\[
\hat{\mathbf{x}}_{t|t-1} = f(\hat{\mathbf{x}}_{t-1}, \mathbf{u}_{t-1})
\]
\[
P_{t|t-1} = F_t P_{t-1} F_t^\top + Q_t
\]

**Update Step:**
\[
K_t = P_{t|t-1} H_t^\top (H_t P_{t|t-1} H_t^\top + R_t)^{-1}
\]
\[
\hat{\mathbf{x}}_{t|t} = \hat{\mathbf{x}}_{t|t-1} + K_t(z_t - h(\hat{\mathbf{x}}_{t|t-1}))
\]

### 🔹 Optimization Ideology  
Linearization errors dominate at high curvature.  
Improved using **adaptive noise covariance tuning**, **iterated EKF**, or **partial Jacobian relinearization**.

---

## 3️⃣ Unscented Kalman Filter (UKF)
### 🔹 Concept  
Eliminates linearization by propagating a set of deterministically chosen sigma points through nonlinear dynamics.

### 🔹 Core Equations  
Sigma point generation:
\[
\chi_i = \hat{\mathbf{x}} + (\sqrt{(n+\lambda)P})_i
\]
Each $\chi_i$ is propagated through motion and measurement functions:
\[
\chi_i' = f(\chi_i, \mathbf{u})
\]
Mean and covariance reconstructed via weighted averaging.

### 🔹 Optimization Ideology  
Outperforms EKF in highly nonlinear domains.  
Key tuning: scaling factor $\alpha$, spread $\kappa$, and weight $\beta$.  
Used for **agile robotics** where smooth transitions matter.

---

## 4️⃣ Quantum Particle Swarm Optimization (QPSO)
### 🔹 Concept  
Meta-heuristic search algorithm inspired by quantum behavior of particles — used here for global pose refinement.

### 🔹 Mathematical Update  
\[
\mathbf{p}_i^{new} = \mathbf{p}_{att} + \beta|\mathbf{mbest} - \mathbf{p}_i| \odot \ln(1/\mathbf{u})
\]
where $\mathbf{u}$ is uniformly distributed in $(0,1)$.

Fitness function minimizes measurement residuals:
\[
J = \sum \frac{(z_{measured} - z_{expected})^2}{\sigma^2}
\]

### 🔹 Optimization Ideology  
No derivative required — robust against local minima.  
Trade-off tuned via contraction–expansion coefficient $\beta$.  
Effective in **terrain-variant mapping** and global optimization.

---

## 5️⃣ Neural Network Residual Learning (LSTM)
### 🔹 Concept  
Uses data-driven residuals to learn systematic prediction errors of model-based filters.

### 🔹 Core Model  
\[
\hat{\mathbf{x}}_{t+1} = f(\hat{\mathbf{x}}_t, \mathbf{u}_t) + \text{NN}([\hat{\mathbf{x}}_t; \mathbf{u}_t])
\]
where the Neural Net (LSTM) learns the residual dynamics.

### 🔹 Optimization Ideology  
Trained on historical motion–observation pairs using **MSE loss**.  
Combines **model interpretability** with **neural adaptability**, suitable for long-term deployment.

---

## 6️⃣ GraphSLAM
### 🔹 Concept  
A full-batch optimization technique — builds a factor graph over poses and constraints.

### 🔹 Optimization Formulation  
\[
\min_{\mathbf{x}} 
\sum_t \| \mathbf{x}_{t+1} - f(\mathbf{x}_t, \mathbf{u}_t) \|_{Q_t}^{2}
+ 
\sum_t \| \mathbf{z}_t - h(\mathbf{x}_t) \|_{R_t}^{2}
\]

Solving via nonlinear least squares (e.g., Gauss-Newton or Levenberg–Marquardt).

### 🔹 Optimization Ideology  
Global consistency through **loop closure constraints**.  
Trade-off: High accuracy vs computational cost (≈52× EKF).  
Best suited for **offline mapping** or survey applications.

---

## 🧩 Summary Insight
| Algorithm | Strength | Limitation | Ideal Use-Case |
|------------|-----------|-------------|----------------|
| Dead-Reckoning | Simple, fast | Drift grows | Low-cost navigation |
| EKF | Real-time optimal | Linearization errors | Warehouse / ground robots |
| UKF | Nonlinear robustness | Moderate computation | Drones / agile bots |
| QPSO | Global optimization | Stochastic, slower | Search & exploration |
| Neural-Net | Learns residuals | Requires training data | Long-term adaptive systems |
| GraphSLAM | Globally optimal | Heavy computation | Mapping & survey robots |

---

### 🧠 Takeaway  
From local incremental estimation to full graph optimization, each algorithm balances **accuracy**, **speed**, and **adaptability** differently.  
Their hybrid integration forms the foundation of modern autonomous SLAM pipelines.

---

**Author:** *Ram Anurag*  
